# [pytorch基础](https://pytorch.apachecn.org/docs/1.2/beginner/blitz/autograd_tutorial.html)
## 1.autograd：自动求导
PyTorch中，所有神经网络的核心是 autograd 包,torch.Tensor 是这个包的核心类。如果设置它的属性 .requires_grad 为 True，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 .backward()，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到.grad属性.

要阻止一个张量被跟踪历史，可以调用 .detach() 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。

### 代码
```python
#autograd
x = torch.ones(4,2,requires_grad=True)
z = x*2
y = z.mean();

y.backward()
print(x)
print(x.grad)
```
### 运行结果
```python
root@e3e15059f685:/data/code/demo/demo1# python pytt.py
tensor([[ 1.,  1.],
        [ 1.,  1.],
        [ 1.,  1.],
        [ 1.,  1.]])
tensor([[ 0.2500,  0.2500],
        [ 0.2500,  0.2500],
        [ 0.2500,  0.2500],
        [ 0.2500,  0.2500]])
```

## 2.一个神经网络的典型训练过程如下：

* 定义包含一些可学习参数(或者叫权重）的神经网络
* 在输入数据集上迭代
* 通过网络处理输入
* 计算损失(输出和正确答案的距离）
* 将梯度反向传播给网络的参数
* 更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient