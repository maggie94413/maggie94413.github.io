<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [nn是什么？](#nn%E6%98%AF%E4%BB%80%E4%B9%88)
  - [一个完整的神经网络过程](#%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%87%E7%A8%8B)
  - [经过nn优化后的神经网络](#%E7%BB%8F%E8%BF%87nn%E4%BC%98%E5%8C%96%E5%90%8E%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)
- [loss相关](#loss%E7%9B%B8%E5%85%B3)
  - [softmax](#softmax)
  - [nnl：negitive log loss](#nnlnegitive-log-loss)
  - [center loss](#center-loss)
- [激活函数：relu：max(0,x)](#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0relumax0x)
- [ignite.engine](#igniteengine)
  - [相关代码](#%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81)
- [度量学习](#%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0)
  - [常用度量学习方法](#%E5%B8%B8%E7%94%A8%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95)
    - [PCA:principel components analysis](#pcaprincipel-components-analysis)
    - [NCA](#nca)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# nn是什么？
```python   
import pickle#序列化数据为python特定格式
import gzip



with gzip.open((PATH / FILENAME).as_posix(), "rb") as f:
        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding="latin-1")

#map() 会根据提供的函数对指定序列做映射。
x_train, y_train, x_valid, y_valid = map(
    torch.tensor, (x_train, y_train, x_valid, y_valid)
)

g_ids = torch.cat(evaluator.state.id_list, dim=0).numpy()#张量转化为numpy
distance = -torch.mm(normalize(q_feats), normalize(g_feats).transpose(0, 1)).numpy()#mm：矩阵相乘 transpose(0,1)不改变矩阵 mul为按位相乘：两个矩阵必须一致


#PyTorch提供创建随机数填充或全零填充张量的方法，我们使用该方法初始化一个简单线性模型的权重和偏置。 这两个都是普通的张量，但它们有一个特殊的附加条件：设置需要计算梯度的参数为True。这样PyTorch就会记录所有与这个张量相关的运算，使其能在反向传播阶段自动计算梯度。

#对于weights而言，由于我们希望初始化张量过程中存在梯度，所以我们在初始化之后设置requires_grad。(注意：尾缀为_的方法在PyTorch中表示这个操作会被立即被执行。）
import math

weights = torch.randn(784, 10) / math.sqrt(784)
weights.requires_grad_()
bias = torch.zeros(10, requires_grad=True)


```

## 一个完整的神经网络过程
```python
from IPython.core.debugger import set_trace

lr = 0.5  # 学习率
epochs = 2  # 训练的轮数

for epoch in range(epochs):
    for i in range((n - 1) // bs + 1):
        #         set_trace()
        start_i = i * bs
        end_i = start_i + bs
        xb = x_train[start_i:end_i]#batch的作用
        yb = y_train[start_i:end_i]
        pred = model(xb)
        loss = loss_func(pred, yb)
        #可替代为：    import torch.nn.functional as F
         #           loss_func = F.cross_entropy
          #          def model(xb):
          #             return xb @ weights + bias

        loss.backward()#反向传播
        with torch.no_grad():#每次grad都重置，不会叠加
            weights -= weights.grad * lr#系数
            bias -= bias.grad * lr#b
            weights.grad.zero_()
            bias.grad.zero_()
```
## 经过nn优化后的神经网络
```python
from torch.utils.data import DataLoader

train_ds = TensorDataset(x_train, y_train)
train_dl = DataLoader(train_ds, batch_size=bs)

model, opt = get_model()

for epoch in range(epochs):
    for xb, yb in train_dl:#dataloader的替换
        pred = model(xb)
        loss = loss_func(pred, yb)

        loss.backward()
        opt.step()#代替手动梯度更新
        opt.zero_grad()

print(loss_func(model(xb), yb))
```

# loss相关
减小loss，训练网络
## softmax
将任意一个向量A，映射到向量B上，使得B中所有的元素都在0到1的范围内，且B中所有元素和为1
具体方法：a->exp(a)
## nnl：negitive log loss
-Y.log().mul(yi).sum()
## center loss
将类间距离变大，需要算跟每个类别的中心的距离

# 激活函数：relu：max(0,x)
作用：引入非线性因素，一些不能线性分类的东西，需要非线性

# ignite.engine

## 相关代码
```python
from ignite.engine import Events#数据，处理函数，返回结果
from ignite.handlers import ModelCheckpoint
from ignite.handlers import Timer#计算事件之间的时间间隔

engine.run(train_loader, max_epochs=cfg.num_epoch)#引擎的用法
```

# 度量学习
度量学习=距离向量学习，用来学习一个距离或者用来降维
## 常用度量学习方法
### PCA:principel components analysis
### NCA