# pytorch函数：

## 1.卷积函数：Conv1d
函数作用：在input上应用卷积

函数原型：

```python
class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)

in_channels(int) – 输入信号的通道。在文本分类中，即为词向量的维度
out_channels(int) – 卷积产生的通道。有多少个out_channels，就需要多少个1维卷积
kernel_size(int or tuple) - 卷积核的尺寸，卷积核的大小为(k,)，第二个维度是由in_channels来决定的，所以实际上卷积大小为kernel_size*in_channels
stride(int or tuple, optional) - 卷积步长
padding (int or tuple, optional)- 输入的每一条边补充0的层数
dilation(int or tuple, `optional``) – 卷积核元素之间的间距
groups(int, optional) – 从输入通道到输出通道的阻塞连接数
bias(bool, optional) - 如果bias=True，添加偏置
```

函数应用举例：
```python
conv1 = nn.Conv1d(in_channels=256，out_channels=100,kernel_size=2)
input = torch.randn(32,35,256)
# batch_size x text_len x embedding_size -> batch_size x embedding_size x text_len
input = input.permute(0,2,1)
out = conv1(input)
print(out.size())
```

## 2.池化函数
### 2.1 avg_pool1d
### 2.2 max_pool1d
### 2.3 lp_pool1d

## 3.非线性激活函数
### 3.1 threshold
### 3.2 relu
### 3.3 hardtanh
### 3.4 hardswish
### 3.5 leaky_relu
### 3.6 logsigmoid
### 3.7 softmax

## 4. 归一化函数
### 4.1 normalize

torchvision.transforms.Normalize(mean, std, inplace=False):
函数作用：对每个channel进行归一化操作：(x-mean)/std;

## 5.线性函数：全连接层
### 5.1 torch.nn.functional.linear(input, weight, bias=None)
```python
import torch as t
from torch import nn

# in_features由输入张量的形状决定，out_features则决定了输出张量的形状 
connected_layer = nn.Linear(in_features = 64*64*3, out_features = 1)

# 假定输入的图像形状为[64,64,3]
input = t.randn(1,64,64,3)

# 将四维张量转换为二维张量之后，才能作为全连接层的输入
input = input.view(1,64*64*3)
print(input.shape)
output = connected_layer(input) # 调用全连接层
print(output.shape)
```
## 6.dropout函数
torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)

## 7.sparse函数

one-hot

## 8.距离函数
pairwise_distance
cosine_similarity
## 9.loss
cross_entropy
## 10.vision函数
interpolate
